# A comparison between named entity recognition models in the biomedical domain 
  
<p align="justify">Biomedical named entity recognition (BioNER) is the task to identify biomedical named entities that are of particular interest for medical research in raw text, and to classify them into predefined categories such as genes and gene products, diseases, chemicals, drugs and etc. Given the exponential growth of biomedical literature in recent years, Bio-NER tools have become increasingly important to support efficiently research work. In the biomedical domain, BioNER is the fundamental task for information extraction (IE), the automated retrieval of specific information related to a selected topic from text. IE tools assists researchers in evaluating the scientific literature, but also play an essential role in a variety of applications that may help to answer many different research questions, ranging from the discovery of drug targets and biomarkers to drug repositioning, the creation of domain specific databases or a state-of-the art resource of a certain disease or therapeutic area (Fleuren et al., 2015).<p>
<p align="justify"> 
Previously proposed methods for NER are dictionary- or rule-based methods and feature-based supervised learning approaches. However, these traditional approaches are heavily reliant on large-scale dictionaries, hand-crafted rules, or well-annotated corpora (gold standard). However, manual annotation and feature engineering by biomedical experts are not often efficient because they involve a considerable amount of engineering skill and domain expertise and require expensive and time-consuming labor (Snow et al., 2008). For this reason, a Deep learning-based approach that is independent of hand-crafted features, has became incredibly popular in recent year. In particular, Deep learning models save a lot effort in that they are capable to automatically learn useful representations and underlying features from raw input data. <p>
<p align="justify">
Distributed representation of texts are used as input data for neural networks. With distributed representations, words are represented as low dimensional real-valued dense vectors where each dimension represents a latent feature. Automatically learned from text, vector representations capture semantic and meaning-related relationships as well as syntactic or grammar-based relationships, which do not explicitly present in the input to NER. The exponential growth of computational power and the opportunity to have available large machine-readable corpora has given the possibility to build pre-trained embeddings that are capable of improving performances in NLP analysis tasks, which often suffer from paucity of data. Pre-trained word embeddings are usually trained on a sufficiently large corpus, saved and reused for solving other tasks. Using pre-trained embeddings can provide benefits such as reduced training time and improved overall performance for several NLP tasks (Yang et al., 2018). GloVe and FastText are commonly used pre-trained embedding models. While GloVe learns word embeddings making use of corpus statistics (building a term co-occurrence matrix), FastText, an extension to Word2Vec (Mikolov et al. in 2013) proposed by Facebook in 2016, learns word vectors from a word prediction task (Bojanowski et al., 2017). <p>
<p align="justify">
There are many factors that make Bio-NER a challenging task due to the nature of text. Biomedical text is characterized by complex multiword entities that could contain numbers and control characters, it exhibits large vocabulary sizes, an excessive usage of abbreviations, and a huge amount of out-of-vocabulary (OOV) words. To address these challenges, seven Machine Learning (ML) models were implemented following a Sequence Tagging (ST) approach. <p>
<p align="justify">
  
## Models and Data
<p align="justify">
Neural models like Recurrent Neural Networks (RNNs) and their variants are the most widely used DL architectures for sequence labelling, because they are able to store information using iterative function loops. RNNs have several properties that make them an attractive choice for sequence labelling: they are flexible in their use of context information (because they can store important informations and forget useless informations); they accept many different types and representations of data; and
they can recognise sequential patterns in the presence of sequential distortions (Graves, 2012). The major problem of RNNs, which has limited their application to real-world ST tasks, is that they cannot capture long-term dependencies because of the vanishing/exploding gradients problem, where the gradients may exponentially decline or grow over long sequences (Pascanu et al., 2013). Long short-term memory networks (LSTMs) are variants of the RNN that incorporates a memory cell to learn long-term dependencies. This special unit is composed of three gates: an input gate, a forget gate, and an output gate. These gates regulate the amount of information for the network to remember and forget for the next time steps. In this way, LSTM are able to deal with the vanishing/exploding gradients problem which is responsable for the reduction in the ability to learn long range relationships in standard RNNs.
Another issue with the standard RNN architecture is that it can only access contextual information in one direction which makes it perfect for task as time-series prediction, but for sequence labelling it is usually advantageous to exploit the context on both directions of the sequences. The solutions came from an extension of traditional LSTMs, which is called Bidirectional LSTM. The principle of Bi-LSTM is to use two separate recurrent layers to scan the data forwards (positive time direction) and backwards (negative time direction), thereby removing the asymmetry between input directions and providing access to all surrounding context (Graves, 2012).<p>
<p align="justify">
Among traditional ML methods, the best results were obtained by systems using CRFs because they are robust and representative algorithms for ST tasks such as bioNER (Li et al., 2008). Considering NER as a ST problem, it can be beneficial to combine Bi-LSTM, which are able to capture infinite amount of context, with a CRF layer on the top of the network to make also the model capable to capture relationship between entities labels. Indeed, it has been shown that jointly decoding label sequences using CRF is more advantageous than decoding each label independently (Cho et al., 2019).<p>
<p align="justify">
Deep learning models require lots of training data for solving a task effectively. Collecting sufficient training data is very costly especially in the biomedical domain because highly reliable corpora are tipically gold-standard corpora (GSCs) which consist of hand-labeled entities. Transfer learning, which focuses on transferring the knowledge across domains, allows us to overcome limitations due to the lack of training data. The intuition behind transfer learning is that if a model is trained on a large and general enough dataset to solve one task, you can take advantage and use the knowledge gained as a starting point to solve a new task. Then, it is possible fine-tuning these pre-trained models to suit specific tasks. This approach is advantageous especially when we work with small datasets, and lead to good results, even better than if we had trained a specific model from scratch.<p>
<p align="justify">
In 2018, it has been introduced ELMo, a new technique for embedding words into real vector space using bi-LSTMs trained on a language modeling objective (Peters et al., 2018). This training task makes use of semi-supervised data by not having to create annotated datasets, but using unstructured text. In language modeling, the goal is to generate the next word in a sentence conditioned to previous words. Given a sequence of tokens (t1, t2,...,tN), a forward language model computes the probability of the sequence by modeling the probability of token tk given the previous tokens (t1,...,tk-1) (Peters et al., 2017). In a neural language model, probability of tk can be computed using a Bi-LSTM network. Specifically, at each position k we obtain two context-dependent representations (forward and backward) and then combine them to obtain the embedding for the token tk (Li et al., 2018). Intuitively, a good language model should capture significant information about not only individual words but also relationships between words based on their positions in sentences. ELMo representations are computed on top of two-layer bidirectional language models with character convolutions and are context-sensitive, meaning that the same word can have different embeddings depending on its contexts. For this reasons, compared to earlier word embedding techniques, such as Word2Vec and GloVe, ELMO is a huge step forward because gives us the possibility to handle polysemy and to compute embeddings even for rare and unseen words. However, ELMo is not fine-tuned to a specific task, the word vectors can be easily added to existing models and significantly improve the state of NER models.<p>
<p align="justify">
The main limitation of earlier language models like ELMO is represented by the inability to take into account both left and right contexts of the target word simultaneously, since the language model objective is generated from left to right. Unlike RNNs, Transformers do not require that the sequential data be processed in order. They are based on the attention mechanism, which allows to concentrate on the most relevant words for predicting the next word in a sentence while ignoring others. With the attention mechanisms, Transformers process an input sequence of words all at once, allowing for much more parallelization than RNNs and then requiring significantly less time to train. 
The fact that Transformers do not rely on sequential processing, allows a more efficiently training on larger datasets than was possible before it were introduced. This has led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers), which has been trained on a huge general language dataset, and can be fine-tuned to specific language tasks (Devlin et al., 2018). BERT replaces the language modeling task with a modified objective called Masked language modeling (MLM, also referred to as a Cloze task (Taylor, 1953)). The idea of a MLM is to reconstruct some corrupted inputs by looking at their contexts. Before feeding word sequences into BERT, words in a sequence are randomly erased and replaced with a special [MASK] token with some small probability. The model then work to predict the original value of the masked words, based on the surrounding non-masked words in the sequence, both to the left and to the right. At the same time, BERT is trained on a Next Sentence Prediction (NSP) task where the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. This training enables the model to understand how sentences relate to each other allowing the model to derive more context. BERT model can be fine-tuned for several NLP tasks, e.g. for a NER task it is possible to feed the output into a classification layer that predicts entities labels.<p>

## Data
<p align="justify">
Experiments for evaluating the performance of several BioNER models were conducted on two datasets. The first is the corpus of the JNLPBA 2004 shared task, which is derived from the GENIA corpus. The second one is the CHEMDNER corpus used for the BioCreative IV shared task (Track 2). Both corpora are public available in the IOB2 annotation format in the MTL-Bioinformatics-2016 [repository](https://github.com/cambridgeltl/MTL-Bioinformatics-2016). According to this schema, tokens are labelled with a B-class tag at the beginning of every sequence that represents an entity, with an I-class tag if the tokens are inside a sequence and with an O tag if the tokens are outside of a sequence that represents an entity.<p>
<p align="justify">
Derived from the GENIA corpus, JNLPBA is a manually annotated collection of articles extracted from the MEDLINE database. 
Compared to the 36 classes of the original corpus, JNLPBA has 5 classes: protein, DNA, RNA, cell line and cell type, and does not contain any nested or discontinuous entities. The training set includes entirely the GENIA corpus, while the test set consists of 404 newly annotated MEDLINE abstracts from the GENIA project.<p>

##### Data Statistics for the JNLPBA corpus
|  | # abstracts | # sentences | # words | 
| :---: | :---: | :---: | :---: | 
| Training Set | 2000 | 18,546 | 472,006 |
| Test Set | 404 | 3,856 | 96,780 |

<p align="justify">
BioCreative IV CHEMDNER (BC-IV) is a collection of PubMed abstracts which contains chemical entity mentions labelled manually by experts in the field, following annotation guidelines specifically defined as part of the BioCreative IV competition. %Since chemical entities of practical importance are those that can be ultimately linked to chemical structure information, rather than general vague chemical concepts, only chemical mentions that can be associated to chemical structure information with at least a certain degree of reliability were annotated. 
No nested annotations or overlapping entity mentions are included. The original fine-grained annotation schema including seven classes was collapsed into one generic class, CHEMICAL.
<p>
  
##### Data Statistics for the CHEMDNER corpus
|  | # abstracts | # sentences | # words | 
| :---: | :---: | :---: | :---: | 
| Training Set | 3500 | 30682 | 891948 |
| Dev Set | 3500 | 30639 | 886324 |
| Test Set | 3000 | 26364 | 766033 |


## Evaluation

<p align="justify"> 
In order to assess and compare NER models, standardized evaluation scores have been employed to evaluate the performance on the biomedical corpora. A frequently used error measures for evaluating NER is the F1-Score, which is computed as the harmonic mean of the Precision and Recall measures. For the evaluation, both exact match and partial match criteria have been taken into account. The exact match metric considers a prediction to be correct only if both its boundaries and its class fully coincide with an annotated entity, while for the partial criterion a prediction is correct as long as a part of the BNE is correctly identified. The partial mode is considered a valid alternative for all that BioNER application in which an exact matches may not be necessary and the only thing that matters is the existence of a BNE. 
 
For the purpose of evaluation it has been used [seqeval](https://github.com/chakki-works/seqeval), a popular Python library for sequence labeling evaluation. <p>
  

## Results

##### Overall performance of the models on JNLPBA

|  | CRF | Bi-LSTM | Bi-LSTM+CRF | FT-BiLSTM+CRF | GL+Char-BiLSTM + CRF | ELMO-BiLSTM + CRF | BERT |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | 
| Precision | 0.69 | 0.68 | 0.69 | 0.69 | 0.70 | 0.64 | 0.69  |
| Recall | 0.69 | 0.68 | 0.70 | 0.74 | 0.75 | 0.77 | 0.77 | 
| F1-score | 0.69 |0.68 | 0.69 | 0.71 | 0.72 | 0.69 | 0.72 |

##### Overall performance of the models on BC-IV

|  | CRF | Bi-LSTM | Bi-LSTM+CRF | FT-BiLSTM+CRF | GL+Char-BiLSTM + CRF | ELMO-BiLSTM + CRF | BERT |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | 
| Precision | 0.86 | 0.85 | 0.77 | 0.82 | 0.83 | 0.77 | 0.89  |
| Recall | 0.73 | 0.74 | 0.83 | 0.77 | 0.82 | 0.87 | 0.87 | 
| F1-score | 0.79 |0.79 | 0.80 | 0.80 | 0.83 | 0.82 | 0.88 |
